{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Distribution\n",
    "\n",
    "*A multinomial generalization of Beta distribution*\n",
    "\n",
    "\n",
    "$f(x_1,\\dots,x_k;\\alpha_1,\\dots,\\alpha_K) = \\dfrac{1}{B(\\alpha)}\\displaystyle \\prod_{i=1}^{K} x_i^{\\alpha_i-1}$<br><br>\n",
    "\n",
    "$B(\\alpha) = \\dfrac{\\displaystyle \\prod_{i=1}^{K}\\Gamma(\\alpha_i)}{\\Gamma(\\displaystyle \\sum_{i=1}^{K}\\alpha_i)}$\n",
    "\n",
    "# Dirichlet Process\n",
    "\n",
    "Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions\n",
    "\n",
    "Dirichlet Process is a distribution over distribution parameterized by $\\alpha$ (dispersion parameter) and $G$(Base Distribution)\n",
    "\n",
    "Dirichlet Process can be demostrated by three models:<br>\n",
    "1.*Chinese Restaurant Process*<br>\n",
    "2.*Polya Urn Process*<br>\n",
    "3.*Stick Breaking Process*<br>\n",
    "\n",
    "Algorithm for simulation of generation of $X_1,X_2,\\dots,$ by Dirichlet Process:\n",
    "\n",
    "1.Draw $X_1$ from the base distribution $G$<br>\n",
    "2.For $n>1$:<br>\n",
    "\n",
    "-With the probability $\\dfrac{\\alpha}{n+\\alpha}$ sample $X_n$ from $G$\n",
    "\n",
    "-With the probability $\\dfrac{n_x}{n+\\alpha}$ set $X_x=x,$  n_x := |${$:X_j=x$ and $j<n$| where $|.|$ denotes the number of elements in the set\n",
    "\n",
    "# Properties of DP\n",
    "\n",
    "Let DP($\\alpha$, $G$ ) be the Dirichlet Process with dispersion parameter $\\alpha$ and base distribution $G$, then<br><br>\n",
    "\n",
    "<center>$\\mathbf{E}_{DP(\\alpha,G)}[x] = \\mathbf{E}_G[x]$<br><br>\n",
    "As $\\alpha \\rightarrow \\infty,  DP(\\alpha,G) \\rightarrow G$</center>\n",
    "\n",
    "\n",
    "# Gibbs Sampling for DPGMM\n",
    "\n",
    "*Base distribution is Gaussain distribution*\n",
    "<br>\n",
    "<center>$p(z_i = k | z_{-i}) \\equiv p(z_i | z_1 \\dots z_{i-1},z_{i+1},\\dots z_m)$</center>\n",
    "\n",
    "<center>$p(z_i = k|z_{-i},\\vec{x},{\\theta_k},\\alpha) $<br><br>\n",
    "$= p(z_i = k|z_{-i},x_i,\\vec{x},\\theta_k,\\alpha) $<br><br>\n",
    "$= p(z_i = k|z_{-i},\\alpha)p(x_i|\\theta_k,\\vec{x})$</center>\n",
    "\n",
    "\\begin{cases}\n",
    "({\\dfrac{n_k}{n+\\alpha}})\\mathcal{N}(x,\\dfrac{n_x}{n+1},\\mathbf{1}),\\space existing\\\\\n",
    "({\\dfrac{\\alpha}{n+\\alpha}})\\mathcal{N}(x,0,1)),\\space new\n",
    "\\end{cases}\n",
    "\n",
    "where,<br>\n",
    "$z_i$ is the group assignment of $x_i$<br>\n",
    "$k$ is the cluster label<br>\n",
    "$\\alpha$ is the dispersion parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbi:hide_in\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import dirichlet, multivariate_normal\n",
    "from scipy import stats\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbi:hide_in\n",
    "def Dir_Dist(a1, a2, a3):\n",
    "    alpha = np.array([a1,a2,a3])\n",
    "    theta = stats.dirichlet(alpha).rvs(1000)\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    ax = plt.gca(projection='3d')\n",
    "    plt.title(r'$\\alpha$ = {}'.format(alpha))\n",
    "    ax.scatter(theta[:, 0], theta[:, 1], theta[:, 2])\n",
    "    ax.view_init(azim=30)\n",
    "    ax.set_xlabel(r'$\\theta_1$')\n",
    "    ax.set_ylabel(r'$\\theta_2$')\n",
    "    ax.set_zlabel(r'$\\theta_3$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca46c94de5241c4b2b3885327f6f864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.05, description='a1', max=2.0, min=0.1), FloatSlider(value=1.05, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.Dir_Dist(a1, a2, a3)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "interact(Dir_Dist, a1=(0.1,2),a2=(0.1,2),a3=(0.1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbi:hide_in\n",
    "def dirichlet_process(h_0, alpha):\n",
    "    \"\"\"\n",
    "    Truncated dirichlet process.\n",
    "    :param h_0: (scipy distribution)\n",
    "    :param alpha: (flt)\n",
    "    :param n: (int) Truncate value.\n",
    "    \"\"\"\n",
    "    n = max(int(5 * alpha + 2), 500)  # truncate the values. \n",
    "    pi = stats.beta(1, alpha).rvs(size=n)\n",
    "    pi[1:] = pi[1:] * (1 - pi[:-1]).cumprod()  # stick breaking process\n",
    "    theta = h_0(size=n)  # samples from original distribution\n",
    "    return pi, theta\n",
    "        \n",
    "def plot_normal_dp_approximation(alpha, n=3):\n",
    "    pi, theta = dirichlet_process(stats.norm.rvs, alpha)\n",
    "    x = np.linspace(-4, 4, 100)\n",
    "    \n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.suptitle(r'Three samples from DP($\\alpha$). $\\alpha$ = {}'.format(alpha))\n",
    "    plt.ylabel(r'$\\pi$')\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    pltcount = int('1' + str(n) + '0')\n",
    "    \n",
    "    for i in range(n):\n",
    "        pltcount += 1\n",
    "        plt.subplot(pltcount)\n",
    "        pi, theta = dirichlet_process(stats.norm.rvs, alpha)\n",
    "        pi = pi * (stats.norm.pdf(0) / pi.max())\n",
    "        plt.vlines(theta, 0, pi, alpha=0.5)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.plot(x, stats.norm.pdf(x))\n",
    "        print(len(theta),len(pi))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217f0b67ceec495381db60667b21cff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='alpha', max=50, min=1), IntSlider(value=3, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_normal_dp_approximation(alpha, n=3)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "interact(plot_normal_dp_approximation, alpha=(1, 50), n=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbi:hide_in\n",
    "def chinese_restaurant_process_clustering(alpha,seed,k_,num_cluster):\n",
    "    #print(\"\\nComputing KMeans with KMeans++ Initialisation....\\n\")\n",
    "    km = KMeans(n_clusters=k_,max_iter=20)\n",
    "    #print(\"Computed KMeans with initialisation!\\n\")\n",
    "    \"\"\"\n",
    "        Here we are adding the dataset - since we are going to be performing gaussian\n",
    "        mixture model clustering using dirichlet processes, we are going to sample the\n",
    "        data points from a mixture of gaussians - just for this demo, we are setting cluster \n",
    "        sizes arbitrarily\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    dataset_1 = np.random.multivariate_normal([5, 5], np.diag([0.5, 0.5]), size=10)\n",
    "    dataset_2 = np.random.multivariate_normal([7.5, 8], np.diag([0.5, 0.5]), size=60)\n",
    "    dataset_3 = np.random.multivariate_normal([10, 12], np.diag([0.5, 0.5]), size=40)\n",
    "    dataset_4 = np.random.multivariate_normal([6,11],np.diag([1,1]),size=50)\n",
    "    dataset_5 = np.random.multivariate_normal([9,11],np.diag([1,1]),size=60)\n",
    "    dataset_6 = np.random.multivariate_normal([10,6],np.diag([0.5,0.5]),size=50)\n",
    "    \n",
    "    dataset_total = np.vstack([dataset_1,dataset_2,dataset_3,dataset_4,dataset_5,dataset_6])\n",
    "    \n",
    "    if num_cluster<=6:\n",
    "        arr = [10,70,110,160,220,270]\n",
    "        dataset_total = dataset_total[:arr[num_cluster-1]]\n",
    "    else:\n",
    "        temp = num_cluster-6\n",
    "        new_cluster = np.random.multivariate_normal([10,9],np.diag([0.75, 0.75]), size=50)\n",
    "        dataset_total = np.vstack([dataset_total,new_cluster])\n",
    "        new_cluster = np.random.multivariate_normal([0,6],np.diag([0.75, 0.75]), size=50)\n",
    "        dataset_total = np.vstack([dataset_total,new_cluster])\n",
    "        \n",
    "    \"\"\"\n",
    "    Just combine all the data points that were generated into a single vector\n",
    "    This is useful since we can later use these as binary indices to find out the \n",
    "    respective clusters\"\"\"\n",
    "\n",
    "    N, D = dataset_total.shape\n",
    "    \"\"\"\n",
    "        Note that N gives the number of samples in the dataset\n",
    "        D is the number of dimensions. Only 2D Case is visualisable\n",
    "        other n-D we can just use the data accordingly\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "        Beginning of the Gaussian Mixture Model \n",
    "        We are going to be assuming that all points belong to the same cluster\n",
    "        initially. Then we shall, decompose the data into clusters or combine them\n",
    "        to have reduced clusters\"\"\"\n",
    "    multi_variate_normal = multivariate_normal # We are using the Higher Dimensional Gaussian\n",
    "    means = []  # This is the list that contains the means of the Gaussians that we fit for each cluster\n",
    "    sigma = np.eye(D) # This is the covariance matrix - for simplicity's sake we are setting the covariance matrix\n",
    "    prec = np.linalg.inv(sigma)  # Fixed precision matrix for all Gaussians\n",
    "    zs = np.zeros([N], dtype=int)  # Latent variables that need to be inferred\n",
    "    C = []  # This is a list of arrays, each of size number of datapoints. The arrays are binary and indicate\n",
    "            # whether a particular element belongs to a cluster or not. Note that each element belongs to one cluster,\n",
    "            # the cluster with its highest probability\n",
    "    samples_per_cluster = []  # Count of each cluster\n",
    "    start_mean = np.ones(D)\n",
    "    start_variance = np.eye(D)\n",
    "    prec0 = np.linalg.inv(np.eye(D))\n",
    "    G = multi_variate_normal(mean=start_mean, cov=start_variance)## Base Distribution\n",
    "    C.append(np.ones(N, dtype=int))\n",
    "    zs[:] = 0        ## Sets all the elements to 0 - assume that all points belonged to the same cluster\n",
    "    samples_per_cluster.append(N)   #Initially all points in same cluster\n",
    "    means.append(G.rvs())           # Add the mean of the starting cluster\n",
    "    K = 1                           # Number of Clusters is now, 1.\n",
    "    #print(\"Gibbs Sampling Started....\\n\")\n",
    "    for iteration in range(20):     # Heuristic - Can be anything of the users choice.\n",
    "        \"\"\"Gibbs Sampling starts here\n",
    "           Note that we are simulating the Chinese Restaurant Process for our model\n",
    "           to cluster the data points\"\"\"\n",
    "        \n",
    "        for i in range(N):          # Gibbs Sampling Begin\n",
    "            \"\"\"\n",
    "                Unassign this particular point.\n",
    "                In each iteration, assign each point to a new \n",
    "                cluster with probability 1/(alpha+n) or add to an existing\n",
    "                cluster with probability proportional to the number of points in every\n",
    "                cluster, i.e., (n_k)/(n+alpha), where n_k is the number of points in cluster k\n",
    "                \"\"\"\n",
    "            \n",
    "            zi = zs[i]              \n",
    "            C[zi][i] = 0\n",
    "            samples_per_cluster[zi] -= 1\n",
    "            \"\"\"\n",
    "                Suppose after unassigning, number of clusters goes to zero.\n",
    "                \"\"\"\n",
    "            if samples_per_cluster[zi] == 0:\n",
    "                zs[zs > zi] -= 1\n",
    "                del C[zi]\n",
    "                del samples_per_cluster[zi]\n",
    "                del means[zi]\n",
    "                K -= 1\n",
    "            probs = np.zeros(K+1)\n",
    "            zs_minus_i = zs[np.arange(len(zs)) != i]\n",
    "            \"\"\"\n",
    "                This is the step where Gibbs Sampling Starts\n",
    "                We normalize the probability since we need to multiply the likelihood \n",
    "                as per max-likelihood principle\n",
    "            \"\"\"\n",
    "            for k in range(K):\n",
    "                nk_minus = zs_minus_i[zs_minus_i == k].shape[0]\n",
    "                crp = nk_minus / (N + alpha - 1)\n",
    "                probs[k] = crp * multi_variate_normal.pdf(dataset_total[i], means[k], sigma)\n",
    "            crp = alpha / (N + alpha - 1)\n",
    "            likelihood = multi_variate_normal.pdf(dataset_total[i], start_mean, start_variance+sigma)\n",
    "            probs[K] = crp*likelihood\n",
    "            probs /= np.sum(probs)\n",
    "            z = np.random.multinomial(n=1, pvals=probs).argmax()\n",
    "            if z == K:\n",
    "                C.append(np.zeros(N, dtype=int))\n",
    "                samples_per_cluster.append(0)\n",
    "                means.append(G.rvs())\n",
    "                K+=1\n",
    "            zs[i] = z\n",
    "            C[z][i] = 1\n",
    "            samples_per_cluster[z] += 1\n",
    "        \"\"\"\n",
    "            Sampling is done here.\n",
    "            Sampling from the conditional distribution\n",
    "        \"\"\"\n",
    "        for k in range(K):\n",
    "            Xk = dataset_total[zs == k]\n",
    "            samples_per_cluster[k] = Xk.shape[0]\n",
    "            lambda_post = prec0 + samples_per_cluster[k]*prec\n",
    "            cov_post = np.linalg.inv(lambda_post)\n",
    "            left = cov_post\n",
    "            right = prec0 @ start_mean + samples_per_cluster[k]*prec @ np.mean(Xk, axis=0)\n",
    "            means_post = left @ right\n",
    "            means[k] = multi_variate_normal.rvs(means_post, cov_post)\n",
    "        \"\"\"\n",
    "            Below we plot - by using the corresponding indices for each cluster.\n",
    "        \"\"\"\n",
    "    #print(\"Gibbs Sampling Done! - Plotting now....\\n\")\n",
    "    km.fit(dataset_total)\n",
    "    predicted = km.predict(dataset_total)\n",
    "    Y=np.zeros(len(dataset_total))\n",
    "    for k in range(K):\n",
    "        Y[C[k]==1]=k+1\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('KMeans++ (k='+str(k_)+')')\n",
    "    plt.xlabel('X Axis')\n",
    "    plt.ylabel('Y Axis')\n",
    "    plt.scatter(dataset_total[:,0],dataset_total[:,1],c=predicted,cmap='plasma')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('CRP Dirichlet Process - Found '+str(K) + ' after 20 iterations')\n",
    "    plt.scatter(dataset_total[:,0],dataset_total[:,1],c=Y,cmap='plasma')\n",
    "    plt.xlabel('X Axis')\n",
    "    plt.ylabel('Y Axis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b379880419d648fb8aace77a92d67a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5000.05, description='alpha', max=10000.0, min=0.1), IntSlider(value=3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.chinese_restaurant_process_clustering(alpha, seed, k_, num_cluster)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "from ipywidgets import interact\n",
    "interact(chinese_restaurant_process_clustering, alpha=(0.1,10000), seed=(1,5),num_cluster=(1,8),k_=(1,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
